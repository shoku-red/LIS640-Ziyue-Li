{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a83ca55",
   "metadata": {},
   "source": [
    "## Milestone 2: Neural Network Baseline and Hyperparameter Optimization\n",
    "\n",
    "LIS 640 - Introduction to Applied Deep Learning\n",
    "\n",
    "Due 3/7/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da321fe4",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "In Milestone 1 you have:\n",
    "1. **Defined a deep learning problem** where AI can make a meaningful impact.\n",
    "2. **Identified three datasets** that fit your topic and justified their relevance.\n",
    "3. **Explored and visualized** the datasets to understand their structure.\n",
    "4. **Implemented a PyTorch Dataset class** to prepare data for deep learning.\n",
    "\n",
    "In Milestone 2 we will take the next step and implement a neural network baseline based on what we have learned in class! For this milestone, please use one of the datasets you picked in the last milestone. If you pick a new one, make sure to do Steps 2 - 4 again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4078e",
   "metadata": {},
   "source": [
    "## **Step 1: Define Your Deep Learning Problem**\n",
    "\n",
    "The first step is to be clear about what you want your model to predict. Is your goal a classification or a regression task? what are the input features and what are you prediction targets y? Make sure that you have a sensible choice of features and a sensible choice of prediction targets y in your dataloader.\n",
    "\n",
    "**Write down one paragraph of justification for how you set up your DataLoader below. If it makes sense to change the DataLoader from Milestone 1, describe what you changed and why:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f1e6a-bb26-404c-b44f-6974eabaf5f5",
   "metadata": {},
   "source": [
    "The dataset I used is about prediction of lung cancer, and the final output is a binary variable. To better apply the model to my dataset, I use one-hotting and labelencoder to change object into int, for most of my data use words like 'High', 'Limited' to describe the evaluation. After that, to make MLP train the data more accurately, I use normalization and standardization to scale column Age and Country. One thing fascinated me was that I learnt how to transform country name to numeric, using target encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb8cc8",
   "metadata": {},
   "source": [
    "## **Step 2: Train a Neural Network in PyTorch**\n",
    "\n",
    "We learned in class how to implement and train a feed forward neural network in pytorch. You can find reference implementations [here](https://github.com/mariru/Intro2ADL/blob/main/Week5/Week5_Lab_Example.ipynb) and [here](https://www.kaggle.com/code/girlboss/mmlm2025-pytorch-lb-0-00000). Tip: Try to implement the neural network by yourself from scratch before looking at the reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3ce23-ce7e-4613-8fc5-2cb552212528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d971bc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 460292 entries, 0 to 460291\n",
      "Data columns (total 27 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   Country                      460292 non-null  float64\n",
      " 1   Age                          460292 non-null  int64  \n",
      " 2   Gender                       460292 non-null  int64  \n",
      " 3   Second_Hand_Smoke            460292 non-null  int64  \n",
      " 4   Air_Pollution_Exposure       460292 non-null  int32  \n",
      " 5   Occupation_Exposure          460292 non-null  int64  \n",
      " 6   Socioeconomic_Status         460292 non-null  int32  \n",
      " 7   Healthcare_Access            460292 non-null  int32  \n",
      " 8   Insurance_Coverage           460292 non-null  int64  \n",
      " 9   Screening_Availability       460292 non-null  int64  \n",
      " 10  Stage_at_Diagnosis           460292 non-null  int32  \n",
      " 11  Treatment_Access             460292 non-null  int64  \n",
      " 12  Clinical_Trial_Access        460292 non-null  int64  \n",
      " 13  Language_Barrier             460292 non-null  int64  \n",
      " 14  Mortality_Risk               460292 non-null  float64\n",
      " 15  5_Year_Survival_Probability  460292 non-null  float64\n",
      " 16  Delay_in_Diagnosis           460292 non-null  int64  \n",
      " 17  Family_History               460292 non-null  int64  \n",
      " 18  Indoor_Smoke_Exposure        460292 non-null  int64  \n",
      " 19  Tobacco_Marketing_Exposure   460292 non-null  int64  \n",
      " 20  Final_Prediction             460292 non-null  int64  \n",
      " 21  Cancer_Type_SCLC             460292 non-null  int32  \n",
      " 22  Rural_or_Urban_Urban         460292 non-null  int32  \n",
      " 23  Smoking_Status_Non-Smoker    460292 non-null  int32  \n",
      " 24  Smoking_Status_Smoker        460292 non-null  int32  \n",
      " 25  Mutation_Type_EGFR           460292 non-null  int32  \n",
      " 26  Mutation_Type_KRAS           460292 non-null  int32  \n",
      "dtypes: float64(3), int32(10), int64(14)\n",
      "memory usage: 77.3 MB\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "lc = pd.read_csv(\"lung_cancer_prediction.csv\")\n",
    "\n",
    "\n",
    "\n",
    "lc['Mutation_Type'] = lc['Mutation_Type'].fillna(lc['Mutation_Type'].mode()[0])\n",
    "lc['Treatment_Access'] = lc['Treatment_Access'].fillna(lc['Treatment_Access'].mode()[0])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "lc['Final_Prediction'] = lc['Final_Prediction'].map({'Yes': 1, 'No': 0})\n",
    "lc['Gender'] = lc['Gender'].map({'Male': 1, 'Female': 0})\n",
    "lc['Second_Hand_Smoke'] = lc['Second_Hand_Smoke'].map({'Yes': 1, 'No': 0})\n",
    "lc['Occupation_Exposure'] = lc['Occupation_Exposure'].map({'Yes': 1, 'No': 0})\n",
    "lc['Insurance_Coverage'] = lc['Insurance_Coverage'].map({'Yes': 1, 'No': 0})\n",
    "lc['Screening_Availability'] = lc['Screening_Availability'].map({'Yes': 1, 'No': 0})\n",
    "lc['Treatment_Access'] = lc['Treatment_Access'].map({'Full': 1, 'Partial': 0})\n",
    "lc['Clinical_Trial_Access'] = lc['Clinical_Trial_Access'].map({'Yes': 1, 'No': 0})\n",
    "lc['Language_Barrier'] = lc['Language_Barrier'].map({'Yes': 1, 'No': 0})\n",
    "lc['Delay_in_Diagnosis'] = lc['Delay_in_Diagnosis'].map({'Yes': 1, 'No': 0})\n",
    "lc['Family_History'] = lc['Family_History'].map({'Yes': 1, 'No': 0})\n",
    "lc['Indoor_Smoke_Exposure'] = lc['Indoor_Smoke_Exposure'].map({'Yes': 1, 'No': 0})\n",
    "lc['Tobacco_Marketing_Exposure'] = lc['Tobacco_Marketing_Exposure'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "lc['Air_Pollution_Exposure'] = encoder.fit_transform(lc['Air_Pollution_Exposure'])\n",
    "lc['Socioeconomic_Status'] = encoder.fit_transform(lc['Socioeconomic_Status'])\n",
    "lc['Healthcare_Access'] = encoder.fit_transform(lc['Healthcare_Access'])\n",
    "lc['Stage_at_Diagnosis'] = encoder.fit_transform(lc['Stage_at_Diagnosis'])\n",
    "\n",
    "lc = pd.get_dummies(lc, columns=['Cancer_Type', 'Rural_or_Urban', 'Smoking_Status', 'Mutation_Type'], drop_first=True)\n",
    "bool_cols = ['Cancer_Type_SCLC', 'Rural_or_Urban_Urban', 'Smoking_Status_Non-Smoker', \n",
    "             'Smoking_Status_Smoker', 'Mutation_Type_EGFR', 'Mutation_Type_KRAS']\n",
    "\n",
    "lc[bool_cols] = lc[bool_cols].astype(int)\n",
    "\n",
    "target_encoder = ce.TargetEncoder(cols=['Country'])\n",
    "lc['Country'] = target_encoder.fit_transform(lc['Country'], lc['Final_Prediction'])\n",
    "\n",
    "lc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d8125c8-3bb8-448f-a4fb-0e2af0197165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler() \n",
    "scaler = StandardScaler()\n",
    "lc[\"Country\"] = scaler.fit_transform(lc[[\"Country\"]])\n",
    "lc[\"Age\"] = scaler.fit_transform(lc[[\"Age\"]])\n",
    "\n",
    "lc[\"Air_Pollution_Exposure\"] = mmscaler.fit_transform(lc[[\"Air_Pollution_Exposure\"]])\n",
    "lc[\"Socioeconomic_Status\"] = mmscaler.fit_transform(lc[[\"Socioeconomic_Status\"]])\n",
    "lc[\"Healthcare_Access\"] = mmscaler.fit_transform(lc[[\"Healthcare_Access\"]])\n",
    "lc[\"Stage_at_Diagnosis\"] = mmscaler.fit_transform(lc[[\"Stage_at_Diagnosis\"]])\n",
    "\n",
    "X = lc.drop('Final_Prediction', axis=1)  \n",
    "Y = lc[\"Final_Prediction\"]\n",
    "#consider using robustscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64869fe8-0c96-49cf-9653-cebfedc8f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloaders: make sure to have a train, validation and a test loader\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "x_train_tensor = torch.tensor(X.values, dtype=torch.float32)  \n",
    "y_train_tensor = torch.tensor(Y.values, dtype=torch.long)\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))  # 80% for training\n",
    "val_size = len(train_dataset) - train_size  # Remaining 20% for validation\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 32  # Set batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482972e1-b4ca-47c7-80bc-6e2bc20ccad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=26, out_features=200, bias=True)\n",
      "    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (4): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (8): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Dropout(p=0.3, inplace=False)\n",
      "    (11): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (12): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "    (14): Dropout(p=0.3, inplace=False)\n",
      "    (15): Linear(in_features=200, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, d_in, d_out, d_hidden, n_layers = 3):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(d_in, d_hidden), nn.BatchNorm1d(d_hidden),\n",
    "            nn.ReLU()]\n",
    "        for layer in range(n_layers):\n",
    "            layers += [nn.Linear(d_hidden, d_hidden), nn.BatchNorm1d(d_hidden), nn.ReLU(), nn.Dropout(p=0.3)]\n",
    "        layers += [nn.Linear(d_hidden, d_out)]\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits  # logits are returned without sigmoid\n",
    "        \n",
    "import torch.optim as optim\n",
    "\n",
    "model = NeuralNetwork(X.shape[1], 1, 200)\n",
    "print(model)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Adjust learning rate as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c5c8a47-c098-4d07-967b-1443842a4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function and the optimizer\n",
    "# Training function\n",
    "def train(model, train_loader, loss_fn, optimizer, device):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device).float()  # Move data to GPU if available\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(batch_X).squeeze()  # Forward pass (ensure output shape matches labels)\n",
    "        loss = loss_fn(outputs, batch_y)  # Compute MSE loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "    \n",
    "    return total_loss / len(train_loader)  # Return average loss per batch\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, val_loader, loss_fn, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device).float()\n",
    "            outputs = model(batch_X).squeeze()  # Ensure output shape matches labels\n",
    "            \n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss  # Return BCE loss (lower is better)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4a2255a-7f26-498f-a545-f49f9bc940c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "  Train Loss (BCE): 0.5069\n",
      "  Validation Loss (BCE): 0.5063\n",
      "Epoch 2/10:\n",
      "  Train Loss (BCE): 0.5032\n",
      "  Validation Loss (BCE): 0.5037\n",
      "Epoch 3/10:\n",
      "  Train Loss (BCE): 0.5025\n",
      "  Validation Loss (BCE): 0.5029\n",
      "Epoch 4/10:\n",
      "  Train Loss (BCE): 0.5023\n",
      "  Validation Loss (BCE): 0.5031\n",
      "Epoch 5/10:\n",
      "  Train Loss (BCE): 0.5020\n",
      "  Validation Loss (BCE): 0.5033\n",
      "Epoch 6/10:\n",
      "  Train Loss (BCE): 0.5018\n",
      "  Validation Loss (BCE): 0.5021\n",
      "Epoch 7/10:\n",
      "  Train Loss (BCE): 0.5017\n",
      "  Validation Loss (BCE): 0.5025\n",
      "Epoch 8/10:\n",
      "  Train Loss (BCE): 0.5015\n",
      "  Validation Loss (BCE): 0.5021\n",
      "Epoch 9/10:\n",
      "  Train Loss (BCE): 0.5014\n",
      "  Validation Loss (BCE): 0.5023\n",
      "Epoch 10/10:\n",
      "  Train Loss (BCE): 0.5013\n",
      "  Validation Loss (BCE): 0.5032\n"
     ]
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust the number of epochs based on performance\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, loss_fn, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, loss_fn, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss (BCE): {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss (BCE): {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca7ad48b-f5e8-4d97-aa46-4a616aa9bdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5032\n",
      "Validation Accuracy: 0.7994\n"
     ]
    }
   ],
   "source": [
    "#test model\n",
    "model.eval() \n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:  #use the validation to test, since I have no test loader\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        outputs = outputs.squeeze(1)\n",
    "        targets=targets.float()\n",
    "        \n",
    "       \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        \n",
    "        preds = (outputs > 0.5).float()\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "\n",
    "val_loss /= len(val_loader)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e90508",
   "metadata": {},
   "source": [
    "## **Step 2 continued: Try Stuff**\n",
    "\n",
    "Use your code above to try different architectures. Make sure to use early stopping! Try adding Dropout and BatchNorm, try different learning rates. How do they affect training and validation performance? \n",
    "\n",
    " **Summarize your observations in a paragraph below:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6d482-31a6-4d42-be3d-fee24f0fb890",
   "metadata": {},
   "source": [
    "Since my dataset has an output of binary variable, I change the loss function to another one. The change of loss seems slow, so I change the learning rate, hoping to make it faster. But it's still hard to lower the validation loss, so I add more layers and epoches. With more training epoches the loss seems to be decreasing, but still hard to reach 0.5 or lower. For the testing part, the accuracy is 0.79, which is not very high. I hope that I can add more functions into the model or try some other methods to optimize the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d368c",
   "metadata": {},
   "source": [
    "## **Step 3: Hyperparameter Optimization with Optuna**\n",
    "\n",
    "As you can see, hyperparameter optimization can be tedious. In class we used [optuna](https://optuna.org/#code_examples) to automate the process. Your next task is to wrap your code from Step 2 into an objective which you can then optimize with optuna. Under the [code exaples](https://optuna.org/#code_examples) there is a tab *PyTorch* which should be helpful as it provides a minimal example on how to wrap PyTorch code inside an objective.\n",
    "\n",
    "**Important: Make sure the model is evaluated on a validation set, not the training data!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3459e10-06b8-4605-9558-334511f96a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "584d1593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 14:24:32,590] A new study created in memory with name: no-name-3820a3d0-d7fe-4a29-8dcc-06fcb9468e35\n",
      "[I 2025-03-06 14:25:59,473] Trial 0 finished with value: 0.5021956781183987 and parameters: {'d_hidden': 128, 'n_layers': 5, 'dropout_p': 0.4405594627998367, 'lr': 0.00011192254646357547, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.5021956781183987.\n",
      "[I 2025-03-06 14:27:05,825] Trial 1 finished with value: 0.5014343377381695 and parameters: {'d_hidden': 384, 'n_layers': 1, 'dropout_p': 0.17266511435810852, 'lr': 9.867612329104415e-05, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.5021956781183987.\n",
      "[I 2025-03-06 14:28:20,477] Trial 2 finished with value: 0.5048189806764775 and parameters: {'d_hidden': 64, 'n_layers': 3, 'dropout_p': 0.28876430627807925, 'lr': 0.0001568931733391693, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.5048189806764775.\n",
      "[I 2025-03-06 14:29:17,137] Trial 3 finished with value: 0.4971452393195883 and parameters: {'d_hidden': 256, 'n_layers': 1, 'dropout_p': 0.43784108123043886, 'lr': 6.534925214206012e-05, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.5048189806764775.\n",
      "[I 2025-03-06 14:31:42,942] Trial 4 finished with value: 0.5078323449368779 and parameters: {'d_hidden': 512, 'n_layers': 4, 'dropout_p': 0.415032311722213, 'lr': 0.0021586453079194693, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.5078323449368779.\n",
      "[I 2025-03-06 14:34:42,783] Trial 5 finished with value: 0.5029839804854057 and parameters: {'d_hidden': 320, 'n_layers': 5, 'dropout_p': 0.47343395757979523, 'lr': 0.0015787998613162508, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.5078323449368779.\n",
      "[I 2025-03-06 14:36:04,785] Trial 6 finished with value: 0.5033990264911767 and parameters: {'d_hidden': 128, 'n_layers': 3, 'dropout_p': 0.3284567292231271, 'lr': 0.00188945321932954, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.5078323449368779.\n",
      "[I 2025-03-06 14:37:20,212] Trial 7 finished with value: 0.5026828707478677 and parameters: {'d_hidden': 64, 'n_layers': 3, 'dropout_p': 0.14448391543191763, 'lr': 7.27914396073523e-05, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.5078323449368779.\n",
      "[I 2025-03-06 14:38:57,659] Trial 8 finished with value: 0.49884873663123785 and parameters: {'d_hidden': 192, 'n_layers': 5, 'dropout_p': 0.32086218905014063, 'lr': 0.002625092733961494, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.5078323449368779.\n",
      "[I 2025-03-06 14:40:50,513] Trial 9 finished with value: 0.49934927558765874 and parameters: {'d_hidden': 256, 'n_layers': 5, 'dropout_p': 0.43589509412038707, 'lr': 2.8107941082892976e-05, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.5078323449368779.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameter: {'d_hidden': 512, 'n_layers': 4, 'dropout_p': 0.415032311722213, 'lr': 0.0021586453079194693, 'optimizer': 'SGD'}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, d_in, d_out, d_hidden, n_layers, dropout_p):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(d_in, d_hidden), nn.BatchNorm1d(d_hidden), nn.ReLU()]\n",
    "        for _ in range(n_layers):\n",
    "            layers += [nn.Linear(d_hidden, d_hidden), nn.BatchNorm1d(d_hidden), nn.ReLU(), nn.Dropout(p=dropout_p)]\n",
    "        layers += [nn.Linear(d_hidden, d_out)]\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x) \n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    d_hidden = trial.suggest_int(\"d_hidden\", 64, 512, step=64)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    dropout_p = trial.suggest_float(\"dropout_p\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
    "\n",
    "    \n",
    "    model = NeuralNetwork(x_train_tensor.shape[1], 1, d_hidden, n_layers,dropout_p)\n",
    "    model = model.to(device)\n",
    "\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) if optimizer_name == \"Adam\" else optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "   \n",
    "    train_loss = train(model, train_loader, loss_fn, optimizer, device)\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:  \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device).float()\n",
    "            output = model(batch_X).cpu().numpy()\n",
    "            preds.extend(output) \n",
    "            true_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    auc = roc_auc_score(true_labels, preds)  \n",
    "    return auc  \n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")  \n",
    "study.optimize(objective, n_trials=10)  \n",
    "\n",
    "print(\"The best parameter:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0b007",
   "metadata": {},
   "source": [
    "## **Step 3 continued: Insights**\n",
    "\n",
    "Did you find the hyperparameter search helpful? Does it help to increase the number of trials in the optimization? Note that so far we have used the simplest version of optuna which has many nice features. Can you discover more useful features by browsing the optuna website? (Hint: try pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc98da1-6bcd-4f28-9060-def0889a5e3d",
   "metadata": {},
   "source": [
    "The hyperparameter search with Optuna has been helpful in finding optimal configurations and improving model performance. Increasing the number of trials can further enhance the search, testing more combinations to find the best solution. Furthermore, it done well both in optimazing and time-saving, if I apply pruning into this optimization, it will work faster than the one with manual optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54312a3",
   "metadata": {},
   "source": [
    "## **Step 4: Final Training**\n",
    "\n",
    "Now that you have found a good hyperparameter setting the validation set is no longer needed. The last step is to combine the training and validation set into a combined training set and retrain the model under the best parameter setting found. Report your final loss on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8b65d47-d930-4c06-8716-b4de51d832ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_params = {'d_hidden': 512, 'n_layers': 4, 'dropout_p': 0.415032311722213, 'lr': 0.0021586453079194693, 'optimizer': 'SGD'}\n",
    "\n",
    "\n",
    "epochs = 10  \n",
    "\n",
    "\n",
    "full_train_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "d_hidden = best_params['d_hidden']\n",
    "n_layers = best_params['n_layers']\n",
    "dropout_p = best_params['dropout_p']\n",
    "lr = best_params['lr']\n",
    "optimizer_choice = best_params['optimizer']\n",
    "\n",
    "\n",
    "model = NeuralNetwork(d_in=26, d_out=1, d_hidden=d_hidden, n_layers=n_layers, dropout_p=dropout_p)\n",
    "model.to(device)\n",
    "\n",
    "if optimizer_choice == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.squeeze(1), targets.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72da67b9-6c9f-45ee-aab1-b73bf100f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5036\n",
      "Validation Accuracy: 0.7994\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:  \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        outputs = outputs.squeeze(1)  \n",
    "        targets = targets.float()\n",
    "        \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        preds = (outputs > 0.5).float()  \n",
    "        correct += (preds == targets).sum().item()  \n",
    "        total += targets.size(0) \n",
    "\n",
    "val_loss /= len(val_loader)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcf39c",
   "metadata": {},
   "source": [
    "## **Final Submission**\n",
    "Upload your submission for Milestone 2 to Canvas. \n",
    "Happy Deep Learning! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
